\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Network study}{7}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{Chapter2}{{2}{7}{Network study}{chapter.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}General Segmentation Purposed Networks}{7}{section.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.1}Experimental Procedure}{7}{subsection.12}}
\newlabel{method}{{2.1.1}{7}{Experimental Procedure}{subsection.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.2}ICNet: Image Cascade Network}{9}{subsection.13}}
\@writefile{toc}{\contentsline {subsubsection}{Introduction}{9}{section*.14}}
\@writefile{toc}{\contentsline {subsubsection}{Description}{10}{section*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Cascade Feature Fusion unit. Two feature maps as inputs. One of the feature maps, F1, coming from a lower resolution branch, has half of the resolution than the other input.\relax }}{11}{figure.caption.16}}
\newlabel{ccf}{{2.1}{11}{Cascade Feature Fusion unit. Two feature maps as inputs. One of the feature maps, F1, coming from a lower resolution branch, has half of the resolution than the other input.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{11}{section*.18}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Sketch of the architecture of ICNet: the three resolution branches the CCFs and the points were the loss is computed. Also the specification of the use of each of the parts in terms of training and testing.\relax }}{12}{figure.caption.17}}
\newlabel{icnet}{{2.2}{12}{Sketch of the architecture of ICNet: the three resolution branches the CCFs and the points were the loss is computed. Also the specification of the use of each of the parts in terms of training and testing.\relax }{figure.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Performance results on validation dataset for the normal structure and the architecture with doubled filters.\relax }}{13}{table.caption.19}}
\newlabel{icnet:table1}{{2.1}{13}{Performance results on validation dataset for the normal structure and the architecture with doubled filters.\relax }{table.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Loss plots for both normal (left) and with doubled filters (right) ICNet architectures. (Green) Validation loss, (Blue) Training Loss, training mIoU (red solid line) and validation mIoU (red crosses). \relax }}{13}{figure.caption.20}}
\newlabel{icnet:filters}{{2.3}{13}{Loss plots for both normal (left) and with doubled filters (right) ICNet architectures. (Green) Validation loss, (Blue) Training Loss, training mIoU (red solid line) and validation mIoU (red crosses). \relax }{figure.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Performance results on validation dataset for the normal structure and the architecture with loss weighting for each setup. Here W1 indicates the inverse frequency weithing and W2 the exponential weighting. Best values enclosed in [].\relax }}{14}{table.caption.21}}
\newlabel{icnet:table2}{{2.2}{14}{Performance results on validation dataset for the normal structure and the architecture with loss weighting for each setup. Here W1 indicates the inverse frequency weithing and W2 the exponential weighting. Best values enclosed in [].\relax }{table.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Performance results on test set for the normal structure with a training of 90k.\relax }}{14}{table.caption.22}}
\newlabel{icnet:table3}{{2.3}{14}{Performance results on test set for the normal structure with a training of 90k.\relax }{table.caption.22}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis and Conclusions}{14}{section*.24}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces First row: ground truth examples. Second row: inference results with best ICNet model\relax }}{15}{figure.caption.23}}
\newlabel{icnet:inference}{{2.4}{15}{First row: ground truth examples. Second row: inference results with best ICNet model\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1.3}SegNet}{17}{subsection.26}}
\@writefile{toc}{\contentsline {subsubsection}{Introduction}{17}{section*.27}}
\@writefile{toc}{\contentsline {subsubsection}{Description}{17}{section*.28}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Sketch of the architecture of SegNet: the encoder, the decoder and the final classification layer, as well as the skip connections between encoder and decoder.\relax }}{17}{figure.caption.29}}
\newlabel{segnet}{{2.5}{17}{Sketch of the architecture of SegNet: the encoder, the decoder and the final classification layer, as well as the skip connections between encoder and decoder.\relax }{figure.caption.29}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Illustration of the use of the saved max-pooling indexes during the upsampling procedure.\relax }}{18}{figure.caption.30}}
\newlabel{segidx}{{2.6}{18}{Illustration of the use of the saved max-pooling indexes during the upsampling procedure.\relax }{figure.caption.30}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results}{19}{section*.31}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Performance results on validation dataset for the original structure and the architecture with doubled filters. Also performance metrics included for the best of the two previous modifications plus data augmentation.\relax }}{20}{table.caption.32}}
\newlabel{segnet:table1}{{2.4}{20}{Performance results on validation dataset for the original structure and the architecture with doubled filters. Also performance metrics included for the best of the two previous modifications plus data augmentation.\relax }{table.caption.32}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Loss plots for both doubled filters (left) and doubled filters with data augmentation (right) Segnet architectures. (Blue) Validation loss, (Orange) Training Loss, training mIoU (red solid line) and validation mIoU (green line). \relax }}{20}{figure.caption.33}}
\newlabel{segnet:filters}{{2.7}{20}{Loss plots for both doubled filters (left) and doubled filters with data augmentation (right) Segnet architectures. (Blue) Validation loss, (Orange) Training Loss, training mIoU (red solid line) and validation mIoU (green line). \relax }{figure.caption.33}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.5}{\ignorespaces Performance results on validation dataset for the doubled filter structure and the same architecture but with loss weighting for each setup. Here W1 indicates the inverse frequency weithing and W2 the exponential weighting (DF, i.e. doubled filters). Between brackets the best perfoming scheme in both mIoU and mean Accuracy per class.\relax }}{21}{table.caption.34}}
\newlabel{segnet:table2}{{2.5}{21}{Performance results on validation dataset for the doubled filter structure and the same architecture but with loss weighting for each setup. Here W1 indicates the inverse frequency weithing and W2 the exponential weighting (DF, i.e. doubled filters). Between brackets the best perfoming scheme in both mIoU and mean Accuracy per class.\relax }{table.caption.34}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces First row: ground truth examples. Second row: inference results with best Segnet model.\relax }}{21}{figure.caption.36}}
\newlabel{segnet:inference}{{2.8}{21}{First row: ground truth examples. Second row: inference results with best Segnet model.\relax }{figure.caption.36}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.6}{\ignorespaces Performance results on test set for the doubled filters structure with a training of 90k.\relax }}{21}{table.caption.35}}
\newlabel{segnet:table3}{{2.6}{21}{Performance results on test set for the doubled filters structure with a training of 90k.\relax }{table.caption.35}{}}
\@writefile{toc}{\contentsline {subsubsection}{Analysis and Conclusions}{21}{section*.37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Plot of the training for the doubled filters structure (90k steps). The values shown are training mIoU (red), validation mIoU (green), training loss (orange) and validation loss (blue). As seen no overfitting is apparent.\relax }}{23}{figure.caption.38}}
\newlabel{segnet:over}{{2.9}{23}{Plot of the training for the doubled filters structure (90k steps). The values shown are training mIoU (red), validation mIoU (green), training loss (orange) and validation loss (blue). As seen no overfitting is apparent.\relax }{figure.caption.38}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces Simple representation of stacked hourglass network.\relax }}{24}{figure.caption.43}}
\newlabel{hourglass:stacked}{{2.10}{24}{Simple representation of stacked hourglass network.\relax }{figure.caption.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Specialized Networks}{24}{section.39}}
\newlabel{specificstudy}{{2.2}{24}{Specialized Networks}{section.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Stacked Hourglass}{24}{subsection.40}}
\@writefile{toc}{\contentsline {subsubsection}{Introduction}{24}{section*.41}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Representation of an hourglass module where each of the boxes represent a residual module.\relax }}{25}{figure.caption.44}}
\newlabel{hourglass:module}{{2.11}{25}{Representation of an hourglass module where each of the boxes represent a residual module.\relax }{figure.caption.44}{}}
\@writefile{toc}{\contentsline {subsubsection}{Description}{25}{section*.42}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.12}{\ignorespaces \textbf  {Left}: residual module used all through the network. \textbf  {Right}: illustration of the intermediate supervision process. The networks branches and a loss is applied to a semantic segmentation output. Then, this output, after having applied convolutions to match the number of channels is added to the main data pipeline. \relax }}{26}{figure.caption.45}}
\newlabel{hourglass:residual}{{2.12}{26}{\textbf {Left}: residual module used all through the network. \textbf {Right}: illustration of the intermediate supervision process. The networks branches and a loss is applied to a semantic segmentation output. Then, this output, after having applied convolutions to match the number of channels is added to the main data pipeline. \relax }{figure.caption.45}{}}
\@writefile{toc}{\contentsline {subsubsection}{Experimental Procedure}{26}{section*.46}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.13}{\ignorespaces Different ground truth resolutions, one for each module. The idea is to learn a progressive refinement of the real ground truth.\relax }}{27}{figure.caption.47}}
\newlabel{hourglass:diffgrounds}{{2.13}{27}{Different ground truth resolutions, one for each module. The idea is to learn a progressive refinement of the real ground truth.\relax }{figure.caption.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.14}{\ignorespaces Location of the different joints. \textbf  {Left}: joints provided by the SURREAL dataset. \textbf  {Right}: central part point location.\relax }}{27}{figure.caption.48}}
\newlabel{hourglass:joints}{{2.14}{27}{Location of the different joints. \textbf {Left}: joints provided by the SURREAL dataset. \textbf {Right}: central part point location.\relax }{figure.caption.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.15}{\ignorespaces Representation of the multitask stacked hourglass. In this case with only one module devoted to the auxiliary task. The whole main network is devoted to semantic segmentation while the parallel module is devoted to joint location.\relax }}{28}{figure.caption.49}}
\newlabel{hourglass:multitask}{{2.15}{28}{Representation of the multitask stacked hourglass. In this case with only one module devoted to the auxiliary task. The whole main network is devoted to semantic segmentation while the parallel module is devoted to joint location.\relax }{figure.caption.49}{}}
\@writefile{toc}{\contentsline {subsubsection}{Results and analysis}{28}{section*.50}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.16}{\ignorespaces Plot of the loss and mIoU performance on the validation set while training. Dashed lines represent loss while solid lines represent mIoU values. Red lines correspond to the different resolutions scheme, blue lines to the scheme with the joint location branch and green lines to the original net.\relax }}{29}{figure.caption.51}}
\newlabel{hourglass:resultsplot}{{2.16}{29}{Plot of the loss and mIoU performance on the validation set while training. Dashed lines represent loss while solid lines represent mIoU values. Red lines correspond to the different resolutions scheme, blue lines to the scheme with the joint location branch and green lines to the original net.\relax }{figure.caption.51}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.7}{\ignorespaces Performance results on validation dataset for the original structure, the original structure plus different GT resolutions and the original structure plus a 2 module head devoted to joint estimation.\relax }}{29}{table.caption.52}}
\newlabel{hourglass:table1}{{2.7}{29}{Performance results on validation dataset for the original structure, the original structure plus different GT resolutions and the original structure plus a 2 module head devoted to joint estimation.\relax }{table.caption.52}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.8}{\ignorespaces Performance results of accuracy (overall and for each class) on the validation set for the original stacked hourglass structure, the original plus different ground truth (GT) resolutions and the original plus the multitask head. \relax }}{30}{table.caption.53}}
\newlabel{hourglass:table2}{{2.8}{30}{Performance results of accuracy (overall and for each class) on the validation set for the original stacked hourglass structure, the original plus different ground truth (GT) resolutions and the original plus the multitask head. \relax }{table.caption.53}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.17}{\ignorespaces First row: ground truth examples. Second row: inference results with best Hourglass model.\relax }}{30}{figure.caption.55}}
\newlabel{hourglass:inference}{{2.17}{30}{First row: ground truth examples. Second row: inference results with best Hourglass model.\relax }{figure.caption.55}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2.9}{\ignorespaces Performance results on test dataset for the original structure.\relax }}{30}{table.caption.54}}
\newlabel{hourglass:table3}{{2.9}{30}{Performance results on test dataset for the original structure.\relax }{table.caption.54}{}}
\@setckpt{Chapters/Chapter2}{
\setcounter{page}{31}
\setcounter{equation}{1}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{2}
\setcounter{subsection}{1}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{17}
\setcounter{table}{9}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{parentequation}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{0}
\setcounter{bookmark@seq@number}{26}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{23}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextrayear}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{section@level}{3}
}
